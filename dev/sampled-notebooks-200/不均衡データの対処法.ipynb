{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. 不均衡データとは\n",
    "\n",
    "> クラスに属するサンプル数に偏りがあるデータを不均衡データという\n",
    "\n",
    "# 2. 不均衡データの問題点\n",
    "> 不均衡データで学習させた場合、すべてのデータを負例と判定したり、再現率/適合率が低くなり結果F値も低くなったりする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years  sales  \\\n",
       "0                   3              0     1                      0  sales   \n",
       "1                   6              0     1                      0  sales   \n",
       "2                   4              0     1                      0  sales   \n",
       "3                   5              0     1                      0  sales   \n",
       "4                   3              0     1                      0  sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HR_DATASET_PATH = 'datasets/HR_comma_sep.csv'\n",
    "hr_df = pd.read_csv(HR_DATASET_PATH)\n",
    "hr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11428\n",
       "1     3571\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_df.left.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taiyou/.pyenv/versions/anaconda3-2.3.0/lib/python3.4/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/taiyou/.pyenv/versions/anaconda3-2.3.0/lib/python3.4/site-packages/pandas/core/indexing.py:415: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データ数: 10499\n",
      "学習データのうち離職した人数: 2533\n",
      "検証データ数: 4500\n",
      "検証データのうち離職した人数: 1038\n",
      "---モデルの評価---\n",
      "正確度: 0.771555555556\n",
      "適合率: 0.509842519685\n",
      "再現率: 0.249518304432\n",
      "F値: 0.335058214748\n",
      "---分割表---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>在籍すると予測</th>\n",
       "      <th>離職すると予測</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>在籍者</th>\n",
       "      <td>3213</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>離職者</th>\n",
       "      <td>779</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     在籍すると予測  離職すると予測\n",
       "在籍者     3213      249\n",
       "離職者      779      259"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC # 線形SVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "use_cols = [\n",
    "    'satisfaction_level',\n",
    "    'last_evaluation',\n",
    "    'number_project',\n",
    "    'average_montly_hours',\n",
    "    'time_spend_company',\n",
    "    'Work_accident',\n",
    "    'promotion_last_5years',\n",
    "]\n",
    "\n",
    "# 離職者数:在籍者数 = 3571:11428の不均衡データ\n",
    "X = hr_df[use_cols]\n",
    "Y = hr_df.left\n",
    "\n",
    "# 標準化\n",
    "transformed_cols = [\n",
    "    'satisfaction_level',\n",
    "    'last_evaluation',\n",
    "    'number_project',\n",
    "    'average_montly_hours',\n",
    "    'time_spend_company',\n",
    "]\n",
    "ss = StandardScaler()\n",
    "ss.fit(X[transformed_cols])\n",
    "X[transformed_cols] = ss.transform(X[transformed_cols])\n",
    "\n",
    "\n",
    "# 交差検証(ホールドアウト法)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, train_size=0.7, random_state=0)\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(x_train, y_train)\n",
    "\n",
    "print('学習データ数: %s' % x_train.shape[0])\n",
    "print('学習データのうち離職した人数: %s' % y_train[y_train == 1].shape[0])\n",
    "print('検証データ数: %s' % x_test.shape[0])\n",
    "print('検証データのうち離職した人数: %s' % y_test[y_test == 1].shape[0])\n",
    "\n",
    "y_pred = linear_svc.predict(x_test)\n",
    "\n",
    "print('---モデルの評価---')\n",
    "print('正確度: %s' % accuracy_score(y_test, y_pred))\n",
    "print('適合率: %s' % precision_score(y_test, y_pred))\n",
    "print('再現率: %s' % recall_score(y_test, y_pred))\n",
    "print('F値: %s' % f1_score(y_test, y_pred))\n",
    "\n",
    "print('---分割表---')\n",
    "confusion_df = pd.DataFrame(confusion_matrix(y_test, y_pred), index=['在籍者', '離職者'], columns=['在籍すると予測', '離職すると予測'])\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. 対処方法\n",
    "\n",
    "- **algorithm-level approaches**: 不均衡を調整する係数をモデルに導入する（コスト関数を調整する）\n",
    "- **data-level approaches**: 正例と負例のサンプル数を調整する\n",
    "    - **アンダーサンプリング**: 多数派データを減少させる\n",
    "    - **オーバーサンプリング**: 少数派データを増加させる\n",
    "    - **ハイブリッド法**: アンダーサンプリングとオーバーサンプリングの両方を行う\n",
    "    \n",
    "以下の具体的なコードでは、`imbalanced-learn`パッケージを用いる。\n",
    "```\n",
    "pip install -U imbalanced-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## チューニング(algorithm-level approaches)\n",
    "\n",
    "クラス重み付けを調整してサンプルサイズが小さい方のクラスの影響力を上げてやる。\n",
    "\n",
    "上記の通り、不均衡データだとサンプルサイズが大きい方のクラス（正例の方が多ければ正方向に、負例の方が多ければ負方向に）に引っ張られてしまうわけです。例えば、学習の際に負例の方が多ければ、検証するときには「全部負」という分類結果になってしまう。\n",
    "\n",
    "そこで、例えばSVMであればマージンのところに正例と負例とで異なる重み付けをかけることでペナルティの量を変える→クラス分類結果にも意図的にバイアスをかけて正例の検出感度を上げる、みたいなことをしてやるというのがここで提案されている解決策です。\n",
    "\n",
    "Pythonのsklearn.svm.SVCならclass_weight引数に、それぞれ正例と負例にかける重み付けの値を指定して入れてやるだけです。\n",
    "\n",
    "値の目安ですが、基本的には2つのクラスのサンプルサイズ比を取り、多い方のクラスを1に固定して少ない方のクラスにはその比率をかける、というのが一般的なようです。それだけ少ない方のクラスに強めの影響力を持たせてやる、という感じですかね。\n",
    "\n",
    "参考サイト\n",
    "\n",
    " - [SVM: Separating hyperplane for unbalanced classes — scikit-learn 0.19.0 documentation](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html)\n",
    " - [不均衡データをSVMでクラス分類するにはどうすれば良いか - 六本木で働くデータサイエンティストのブログ](http://tjo.hatenablog.com/entry/2014/10/09/224106)\n",
    "\n",
    "```python\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, train_size=0.7, random_state=0)\n",
    "\n",
    "# ※leftが0が多い!\n",
    "CLASS_WEIGHT = {1: len(Y[Y == 1])/len(Y[Y == 0])}\n",
    "linear_svc = LinearSVC(class_weight=CLASS_WEIGHT)\n",
    "linear_svc.fit(x_train, y_train)\n",
    "\n",
    "print('学習データ数: %s' % x_train.shape[0])\n",
    "print('学習データのうち離職した人数: %s' % y_train[y_train == 1].shape[0])\n",
    "print('検証データ数: %s' % x_test.shape[0])\n",
    "print('検証データのうち離職した人数: %s' % y_test[y_test == 1].shape[0])\n",
    "\n",
    "y_pred = linear_svc.predict(x_test)\n",
    "\n",
    "print('---モデルの評価---')\n",
    "print('正確度: %s' % accuracy_score(y_test, y_pred))\n",
    "print('適合率: %s' % precision_score(y_test, y_pred))\n",
    "print('再現率: %s' % recall_score(y_test, y_pred))\n",
    "print('F値: %s' % f1_score(y_test, y_pred))\n",
    "\n",
    "print('---分割表---')\n",
    "confusion_df = pd.DataFrame(confusion_matrix(y_test, y_pred), index=['在籍者', '離職者'], columns=['在籍すると予測', '離職すると予測'])\n",
    "confusion_df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## アンダーサンプリング: 多数派データを減少させる\n",
    "\n",
    " - 参考コード: https://github.com/scikit-learn-contrib/imbalanced-learn/tree/master/examples/under-sampling\n",
    "\n",
    "### 1. 多数派データをランダムに抽出する\n",
    " - **<font color=\"blue\">メリット</font>**: `sample()`を使うだけ簡単に実装できる\n",
    " - **<font color=\"red\">デメリット</font>**: 元の多数派データの特徴量の分布が崩れてしまう可能性がある\n",
    "\n",
    "```python\n",
    "X1 = hr_df[hr_df.left == 1][use_cols]\n",
    "X0 = hr_df[hr_df.left == 0][use_cols].sample(len(X1))\n",
    "X = pd.concat([X1, X0])\n",
    "Y = hr_df.loc[X.index, 'left']\n",
    "```\n",
    "もしくは、\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X = hr_df[use_cols]\n",
    "Y = hr_df.left\n",
    "\n",
    "# Apply the random under-sampling\n",
    "rus = RandomUnderSampler(return_indices=True, random_state=0)\n",
    "x_resampled, y_resampled, idx_resampled = rus.fit_sample(X, Y)\n",
    "```\n",
    "\n",
    "RandomUnderSamplerの引数\n",
    "```python\n",
    "RandomUnderSampler(\n",
    "         ratio='auto',\n",
    "         return_indices=False,\n",
    "         random_state=None,\n",
    "         replacement=False\n",
    "     )\n",
    "```\n",
    " - ratio : str, dict, or callable, optional (default='auto'). Ratio to use for resampling the data set.\n",
    " - return_indices : bool, optional (default=False). Whether or not to return the indices of the samples randomly selected from the majority class.\n",
    " - random_state : int, RandomState instance or None, optional (default=None)\n",
    " - replacement : boolean, optional (default=False). Whether the sample is with or without replacement.\n",
    "\n",
    "### 2. Extraction of majority-minority Tomek links\n",
    "```python\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# remove Tomek links\n",
    "tl = TomekLinks(return_indices=True)\n",
    "X_resampled, y_resampled, idx_resampled = tl.fit_sample(X, Y)\n",
    "```\n",
    "\n",
    "### 3. Under-sampling with Cluster Centroids\n",
    "```python\n",
    "\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# Apply Cluster Centroids\n",
    "cc = ClusterCentroids()\n",
    "X_resampled, y_resampled = cc.fit_sample(X, Y)\n",
    "\n",
    "# Use hard voting instead of soft voting\n",
    "cc = ClusterCentroids(voting='hard')\n",
    "X_resampled, y_resampled = cc.fit_sample(X, Y)\n",
    "```\n",
    "\n",
    "### 4. NearMiss-(1 & 2 & 3)\n",
    "### 5. Condensend Nearest Neighbour\n",
    "### 6. One-Sided Selection\n",
    "### 7. Neighboorhood Cleaning Rule\n",
    "```python\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "\n",
    "ncr = NeighbourhoodCleaningRule(random_state=0)\n",
    "X_res, y_res = ncr.fit_sample(X, Y)\n",
    "```\n",
    "\n",
    "### 8. Edited Nearest Neighbours\n",
    "### 9. Instance Hardness Threshold\n",
    "### 10. Repeated Edited Nearest Neighbours\n",
    "### 11. AllKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## オーバーサンプリング: 少数派データを増加させる\n",
    "\n",
    " - 参考コード: https://github.com/scikit-learn-contrib/imbalanced-learn/tree/master/examples/over-sampling\n",
    "\n",
    "### 1. 少数派データの複製(Random minority over-sampling with replacement)\n",
    " - **<font color=\"blue\">メリット</font>**: ゲロ簡単\n",
    " - **<font color=\"red\">デメリット</font>**: 過学習を引き起こしやすくなる\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Apply the random over-sampling\n",
    "ros = RandomOverSampler()\n",
    "X_resampled, y_resampled = ros.fit_sample(X, Y)\n",
    "```\n",
    "\n",
    "### 2. SMOTE - Synthetic Minority Over-sampling Technique\n",
    "\n",
    "データセットにおけるそれぞれのデータの k 最近傍を基に新たなデータを生成する。\n",
    "\n",
    " - https://www.jair.org/media/953/live-953-2037-jair.pdf\n",
    " - http://qiita.com/shima_x/items/370587304ef17e7a61b8#over-sampling\n",
    "\n",
    "**SMOTEのアルゴリズム**$(T, N, k)$\n",
    "\n",
    "入力: \n",
    " - $T=$少数派クラスのサンプル数(Number of minority class samples)\n",
    " - $N=$SMOTEの量(Amount of SMOTE)\n",
    " - $k=$最近傍数(Number of nearest neighbors)\n",
    "\n",
    "出力: $(N/100) \\times T$ 合成した少数派クラスのサンプル数\n",
    "\n",
    "(∗ If N is less than 100%, randomize the minority class samples as only a random　percent of them will be SMOTEd. ∗)\n",
    "$\n",
    "if\\quad N<100\\\\ \n",
    "\\qquad then\\quad 少数派データをランダム化(Randomize\\quad the\\quad T\\quad minority\\quad class\\quad samples)\\\\ \\qquad T=(N/100)\\ast T\\\\\n",
    "\\qquad N=100\\\\ \n",
    "endif\n",
    "$\n",
    "$\n",
    "\\\\ \n",
    "N=(int)(N/100)(∗The\\quad amount\\quad of\\quad SMOTE\\quad is\\quad assumed\\quad to\\quad be\\quad inintegral\\quad multiples\\quad of\\quad 100.∗)\\\\ \n",
    "k=Number\\quad of\\quad nearest\\quad neighbors\\\\ \n",
    "numattrs=Number\\quad of\\quad attributes\\\\ \n",
    "Sample[][]:array\\quad for\\quad original\\quad minority\\quad class\\quad samples\\\\ \n",
    "newindex:keeps\\quad a\\quad count\\quad of\\quad number\\quad of\\quad synthetic\\quad samples\\quad generated,\\quad initialized\\quad to\\quad 0\\\\ \n",
    "Synthetic[][]:array\\quad for\\quad synthetic\\quad samples\n",
    "$\n",
    "\n",
    "少数派データのサンプルごとにk最近傍を計算(Compute k nearest neighbors for each minority class sample only)\n",
    "\n",
    "$\n",
    "for\\quad i←1toT\\\\ \n",
    "\\qquad Compute\\quad k\\quad nearestneighbors\\quad for\\quad i,\\quad and\\quad save\\quad the\\quad indices\\quad in\\quad the\\quad nnarray\\\\\n",
    "\\qquad Populate(N,i,nnarray)\\\\ \n",
    "endfor\n",
    "$\n",
    "\n",
    "Populate(N, i, nnarray) (∗ Function to generate the synthetic samples. ∗)\n",
    "\n",
    "$\n",
    "while\\quad N\u0004\\neq 0\\\\ \n",
    "\\qquad 1からkからランダムな値を選び,その値をnnとする.\\\\ \n",
    "\\qquad このステップはiデータのk最近傍データ(k\\quad nearest\\quad neighbors\\quad of\\quad i)のうち一つだけ選ぶ\\\\ \n",
    "\\qquad for\\quad attr←1\\quad to\\quad numattrs\\\\ \n",
    "\\qquad \\qquad Compute:dif=Sample[nnarray[nn]][attr]-Sample[i][attr]\\\\ \n",
    "\\qquad \\qquad Compute:gap=random\\quad number\\quad between\\quad 0\\quad and\\quad 1\\\\ \n",
    "\\qquad \\qquad Synthetic[newindex][attr]=Sample[i][attr]+gap∗dif\\\\ \n",
    "\\qquad endfor\\\\ \n",
    "\\qquad newindex++\\\\ \n",
    "\\qquad N=N−1\\\\ \n",
    "endwhile\\\\ \n",
    "return(∗EndofPopulate.∗)\n",
    "$\n",
    "\n",
    " - **<font color=\"blue\">メリット</font>**: \n",
    " - **<font color=\"red\">デメリット</font>**: \n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply regular SMOTE\n",
    "sm = SMOTE(kind='regular')\n",
    "X_res, y_res = sm.fit_sample(X, Y)\n",
    "```\n",
    "\n",
    "### 3. bSMOTE(1 & 2) - Borderline SMOTE of types 1 and 2\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(kind='borderline1')\n",
    "X_res, y_res = sm.fit_sample(X, Y)\n",
    "```\n",
    "or\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(kind='borderline2')\n",
    "X_res, y_res = sm.fit_sample(X, Y)\n",
    "```\n",
    "\n",
    "### 4. SVM SMOTE - Support Vectors SMOTE\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(kind='svm')\n",
    "X_res, y_res = sm.fit_sample(X, Y)\n",
    "```\n",
    "\n",
    "### 5. ADASYN - Adaptive synthetic sampling approach for imbalanced learning\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# default options\n",
    "ada = ADASYN(ratio='auto',\n",
    "                 random_state=None,\n",
    "                 n_neighbors=5,\n",
    "                 n_jobs=1)\n",
    "X_res, y_res = ada.fit_sample(X, Y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 参考文献\n",
    " - [不均衡データのクラス分類](https://www.slideshare.net/sfchaos/ss-11307051)\n",
    " - [不均衡データにおけるsampling - Qiita](http://qiita.com/shima_x/items/370587304ef17e7a61b8)\n",
    " - [不均衡データに対するClassification - Qiita](http://qiita.com/ryouta0506/items/619d9ac0d80f8c0aed92)\n",
    " - [Welcome to imbalanced-learn documentation! — imbalanced-learn 0.3.0 documentation](http://contrib.scikit-learn.org/imbalanced-learn/stable/)\n",
    " - [scikit-learn-contrib/imbalanced-learn: Python module to perform under sampling and over sampling with various techniques.](https://github.com/scikit-learn-contrib/imbalanced-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
