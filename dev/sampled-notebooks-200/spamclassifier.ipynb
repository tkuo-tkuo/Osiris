{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "header = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n",
    "    \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n",
    "    \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
    "    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\",\n",
    "    \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\",\n",
    "    \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\",\n",
    "    \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\",\n",
    "    \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\",\n",
    "    \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\",\n",
    "    \"char_freq_#\", \"capital_run_length_average\", \"capital_run_length_longest\", \"capital_run_length_total\",\n",
    "    \"is_spam\"\n",
    "]\n",
    "source_data_df = pd.read_csv(data_url, names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail     ...       char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000     ...       4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413     ...          0.038575     0.139030   \n",
       "std           0.278616        0.644755     ...          0.243471     0.270355   \n",
       "min           0.000000        0.000000     ...          0.000000     0.000000   \n",
       "25%           0.000000        0.000000     ...          0.000000     0.000000   \n",
       "50%           0.000000        0.000000     ...          0.000000     0.065000   \n",
       "75%           0.000000        0.160000     ...          0.000000     0.188000   \n",
       "max           5.260000       18.180000     ...          4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total      is_spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_spam                       1.000000\n",
      "word_freq_your                0.383234\n",
      "word_freq_000                 0.334787\n",
      "word_freq_remove              0.332117\n",
      "char_freq_$                   0.323629\n",
      "word_freq_you                 0.273651\n",
      "word_freq_free                0.263215\n",
      "word_freq_business            0.263204\n",
      "capital_run_length_total      0.249164\n",
      "word_freq_our                 0.241920\n",
      "char_freq_!                   0.241888\n",
      "word_freq_receive             0.234529\n",
      "word_freq_over                0.232604\n",
      "word_freq_order               0.231551\n",
      "word_freq_money               0.216111\n",
      "capital_run_length_longest    0.216097\n",
      "word_freq_internet            0.206808\n",
      "word_freq_email               0.204208\n",
      "word_freq_all                 0.196988\n",
      "word_freq_addresses           0.195902\n",
      "word_freq_credit              0.189761\n",
      "word_freq_mail                0.138962\n",
      "word_freq_people              0.132927\n",
      "word_freq_make                0.126208\n",
      "capital_run_length_average    0.109999\n",
      "word_freq_font                0.091860\n",
      "char_freq_#                   0.065067\n",
      "word_freq_report              0.060027\n",
      "word_freq_3d                  0.057371\n",
      "word_freq_will                0.007741\n",
      "word_freq_address            -0.030224\n",
      "word_freq_parts              -0.031035\n",
      "word_freq_table              -0.044679\n",
      "char_freq_;                  -0.059630\n",
      "char_freq_[                  -0.064709\n",
      "word_freq_direct             -0.064801\n",
      "word_freq_conference         -0.084020\n",
      "char_freq_(                  -0.089672\n",
      "word_freq_project            -0.094594\n",
      "word_freq_cs                 -0.097375\n",
      "word_freq_415                -0.112754\n",
      "word_freq_857                -0.114214\n",
      "word_freq_data               -0.119931\n",
      "word_freq_pm                 -0.122831\n",
      "word_freq_telnet             -0.126912\n",
      "word_freq_lab                -0.133523\n",
      "word_freq_original           -0.135664\n",
      "word_freq_technology         -0.136134\n",
      "word_freq_meeting            -0.136615\n",
      "word_freq_re                 -0.140408\n",
      "word_freq_edu                -0.146138\n",
      "word_freq_85                 -0.149225\n",
      "word_freq_650                -0.158800\n",
      "word_freq_labs               -0.171095\n",
      "word_freq_1999               -0.178045\n",
      "word_freq_george             -0.183404\n",
      "word_freq_hpl                -0.232968\n",
      "word_freq_hp                 -0.256723\n",
      "Name: is_spam, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Covariance matrix\n",
    "correlation_matrix = source_data_df.corr()\n",
    "correlation_values = correlation_matrix[\"is_spam\"].sort_values(ascending=False)\n",
    "print(correlation_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('is_spam', 1.0)\n",
      "('word_freq_your', 0.3832338192835756)\n",
      "('word_freq_000', 0.3347870388457389)\n",
      "('word_freq_remove', 0.3321174156141586)\n",
      "('char_freq_$', 0.3236288064980446)\n",
      "('word_freq_you', 0.2736512865572106)\n",
      "('word_freq_free', 0.26321469903669603)\n",
      "('word_freq_business', 0.2632039828223917)\n",
      "('word_freq_hp', 0.2567229163126616)\n",
      "('capital_run_length_total', 0.24916412436334212)\n",
      "('word_freq_our', 0.2419204377148765)\n",
      "('char_freq_!', 0.24188836701122152)\n",
      "('word_freq_receive', 0.23452927138249544)\n",
      "('word_freq_hpl', 0.23296768680660979)\n",
      "('word_freq_over', 0.232604300172187)\n",
      "('word_freq_order', 0.23155143128601532)\n",
      "('word_freq_money', 0.21611098224724426)\n",
      "('capital_run_length_longest', 0.21609669406935564)\n",
      "('word_freq_internet', 0.20680847576170805)\n",
      "('word_freq_email', 0.20420813879714014)\n",
      "('word_freq_all', 0.1969879726831455)\n",
      "('word_freq_addresses', 0.19590245671258405)\n",
      "('word_freq_credit', 0.1897611485080607)\n",
      "('word_freq_george', 0.1834040051933605)\n",
      "('word_freq_1999', 0.17804545978118416)\n",
      "('word_freq_labs', 0.1710945875471117)\n",
      "('word_freq_650', 0.15880024911827956)\n",
      "('word_freq_85', 0.14922521446054307)\n",
      "('word_freq_edu', 0.1461381860642688)\n",
      "('word_freq_re', 0.14040846452155356)\n",
      "('word_freq_mail', 0.13896211329514088)\n",
      "('word_freq_meeting', 0.13661537156641534)\n",
      "('word_freq_technology', 0.13613420484603733)\n",
      "('word_freq_original', 0.13566428089524346)\n",
      "('word_freq_lab', 0.13352318779632708)\n",
      "('word_freq_people', 0.13292672586847548)\n",
      "('word_freq_telnet', 0.1269121577048615)\n",
      "('word_freq_make', 0.1262075747276322)\n",
      "('word_freq_pm', 0.12283065400904399)\n",
      "('word_freq_data', 0.11993101263694417)\n",
      "('word_freq_857', 0.11421403277162386)\n",
      "('word_freq_415', 0.11275418527472239)\n",
      "('capital_run_length_average', 0.10999914323015575)\n",
      "('word_freq_cs', 0.09737483208339674)\n",
      "('word_freq_project', 0.09459404198218023)\n",
      "('word_freq_font', 0.09186009631515232)\n",
      "('char_freq_(', 0.08967198136960924)\n",
      "('word_freq_conference', 0.08401980167055988)\n",
      "('char_freq_#', 0.06506672853837846)\n",
      "('word_freq_direct', 0.06480120227695867)\n",
      "('char_freq_[', 0.06470933654508174)\n",
      "('word_freq_report', 0.060027316399873264)\n",
      "('char_freq_;', 0.05962966689696018)\n",
      "('word_freq_3d', 0.05737138961858829)\n",
      "('word_freq_table', 0.044678563084814875)\n",
      "('word_freq_parts', 0.03103528941229571)\n",
      "('word_freq_address', 0.030223579212021543)\n",
      "('word_freq_will', 0.007740737297956368)\n"
     ]
    }
   ],
   "source": [
    "# Most correlated features\n",
    "feature_correlation_pairs = []\n",
    "for feature, value in correlation_values.items():\n",
    "    feature_correlation_pairs.append((feature, abs(value)))\n",
    "    \n",
    "sorted_feature_correlation_pairs = sorted(feature_correlation_pairs, key=lambda pair: pair[1], reverse=True)    \n",
    "\n",
    "most_correlated_features = [\n",
    "    feature_correlation_pair[0]\n",
    "    for feature_correlation_pair in sorted_feature_correlation_pairs \n",
    "]\n",
    "for sorted_feature_correlation_pair in sorted_feature_correlation_pairs:\n",
    "    print(sorted_feature_correlation_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_data_df = source_data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert pandas to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and Y are the input and output of the classifier algorithm\n",
    "y = ready_data_df[\"is_spam\"].astype(int).values\n",
    "\n",
    "ready_data_df.drop(\"is_spam\", axis=1, inplace=True)\n",
    "\n",
    "# X is the training and test data (we sill use cv validation to test the accuracy of each algorithm)\n",
    "X = ready_data_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierRunner(object):\n",
    "    \n",
    "    def __init__(self, pipeline, parameters, debug=False, num_folds=5):\n",
    "        self.pipeline = pipeline\n",
    "        self.parameters = parameters\n",
    "        self.grid_search = GridSearchCV(self.pipeline, self.parameters, cv=num_folds)\n",
    "        self.debug = debug\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.grid_search.fit(X, y)\n",
    "\n",
    "    @property\n",
    "    def best_params(self):\n",
    "        return self.grid_search.best_params_\n",
    "        \n",
    "    def get_scores(self, X, y, n_splits=5, test_size=0.2, random_seed=0):\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "        scores = cross_val_score(self.grid_search.best_estimator_, X, y, cv=cv)\n",
    "        return scores\n",
    "    \n",
    "    @property\n",
    "    def feature_importances(self):\n",
    "        classifier_step_index = 0\n",
    "        for step_name, step_process in self.grid_search.best_estimator_.steps:\n",
    "            if step_name == \"classifier\":\n",
    "                break\n",
    "            classifier_step_index += 1\n",
    "        feature_importances = self.grid_search.best_estimator_.steps[classifier_step_index][1].feature_importances_\n",
    "        return sorted(zip(feature_importances, selected_features), reverse=True)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        prediction = self.grid_search.predict(X_test)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('reduce_dimensions', PCA()),\n",
    "        ('minmaxscaler', scaler()),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5, 7, 10, 15, 20, 25, 30, 40, 50]\n",
    "parameters = {\n",
    "    'reduce_dimensions__random_state': [1, 2, 3, 4, 5],\n",
    "    'reduce_dimensions__n_components': [10, 25, 50, None],\n",
    "    'classifier__n_neighbors': k_values, \n",
    "    'classifier__weights': [\"uniform\", \"distance\"],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "classifier = ClassifierRunner(pipeline=pipeline, parameters=parameters)\n",
    "\n",
    "classifier.fit(X, y)\n",
    "\n",
    "print (\"Best parameters found: \")\n",
    "print (classifier.best_params)\n",
    "\n",
    "scores = classifier.get_scores(X, y)\n",
    "print (\"Expected performance: {:.2f}% (+/-{:.2f}).\".format(np.mean(scores)*100., np.std(scores)*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('reduce_dimensions', PCA()),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'reduce_dimensions__random_state': [1, 2, 3, 4, 5],\n",
    "    'reduce_dimensions__n_components': [10, 25, 50, None],\n",
    "    'classifier__max_depth' : [4, 6, 12, 20, None],\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__n_estimators': [2, 10, 50, 100],\n",
    "    'classifier__max_features': ['sqrt', 'auto', 'log2', None],\n",
    "    'classifier__min_samples_split': [2, 3, 10],\n",
    "    'classifier__min_samples_leaf': [1, 3, 10],\n",
    "    'classifier__bootstrap': [True, False],\n",
    "    'classifier__n_jobs': [-1]\n",
    "}\n",
    "\n",
    "classifier = ClassifierRunner(pipeline=pipeline, parameters=parameters)\n",
    "\n",
    "classifier.fit(X, y)\n",
    "\n",
    "print (\"Best parameters found: \")\n",
    "print (classifier.best_params)\n",
    "\n",
    "feature_importances = classifier.feature_importances\n",
    "print(\"Selected features by importance: {}\".format(feature_importances))\n",
    "\n",
    "scores = classifier.get_scores(X, y)\n",
    "print (\"Expected performance: {:.2f}% (+/-{:.2f}).\".format(np.mean(scores)*100., np.std(scores)*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
